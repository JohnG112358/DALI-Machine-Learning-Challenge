{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DALI 2024 Winter Application - Machine Learning Track\n",
        "### John Guerrerio\n",
        "\n",
        "We now develop models to predict if the profit of a purchase is above or below the median purchase profit (as determined by our exploratory analysis).  A superstore would likely have access to all features in the Superstore.csv dataset at order time except profit; that would likely have to be calculated later.  Therefore, it would be helpful to the superstore to have a lightweight model than can predict weather a purchase will be above or below the median profit at order time.\n",
        "\n",
        "Please note that deep learing is a promising approach for this classification problem and would likely perform better than the models I present here.  However, I ran out of GPU compute units on colab after training my models for predictCategory.ipynb, so I cannot train deep learning models for this task.\\\n",
        "\\\n",
        "Areas for expansion:\n",
        "- Try additional models (e.g. random forest)\n",
        "- Try a deep learning approach\n",
        "- Develop a data pipeline to fill in missing values in the dataset before we train models (for the models in this file, we had enough data to comfortably train even after removing the missing values.  However, more data couldn't hurt)"
      ],
      "metadata": {
        "id": "lCGdqLSLgc53"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5YQK1Zx8SoQs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preapre the Dataset\n",
        "\n",
        "We first need to prepare the dataset for machine learning.  This involves loading it, generating correct class labels (above or below the median), generating feature vectors for each entry, and splitting it into train, validation and test sets.\n",
        "\n",
        "Splitting the dataset into train, validation and test sets is important to prevent data leakage.  The purpose of each set is as follows:\n",
        "\n",
        "- Train: The data to train the model on, refining it over time.\n",
        "- Validation: The dataset on which to optimize performance when tuning hyperparameters.\n",
        "- Test: The dataset to evaluate the model on after training and hyperparameter tuning.  This gives an indication of the model's real-world performace.\n",
        "\n",
        "If we train on data outside the train dataset, we will artificially inflate our model's performance because the model will have seen the data in that set before.  Similarly, if we optimize our test set performance when tuning hyperparameters, the test set performance will be artifially inflated and we won't have good means of measuring the model's real world performance.  For this project, we use a 70-15-15 train test validation split."
      ],
      "metadata": {
        "id": "GrPlJrDmidv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Superstore.csv')"
      ],
      "metadata": {
        "id": "jOG6QNeITpxD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results of correlation.ipynb, we choose 5 features that appear to be correlated most strongly with profit: region, product category, product sub-category, quantity, and discount.  We drop rows that contain null values for any of these columns.  We are assuming data entries are missing completely at random, so this should not introduce bias into the dataset.  "
      ],
      "metadata": {
        "id": "zV_T4BxfjPJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset=[\"Region\", \"Category\", \"Sub-Category\", \"Quantity\", \"Discount\"], inplace=True)\n",
        "print(\"Number of rows: \" + str(len(df.index))) # make sure we still have enough data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3z4IsDFUBdZ",
        "outputId": "65780fb3-a0a5-40c3-89df-4a60fea1fb82"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 5923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MEDIAN = 8.662 # from the exploratory analysis file\n",
        "RANDOM_STATE = 42 # random seed to ensure results are reproducible"
      ],
      "metadata": {
        "id": "7Tz6ZVE_fqDT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell generates a feature vector for each row and generates the label for that row (1 for above median profit, 0 for less than or equal to the median profit)"
      ],
      "metadata": {
        "id": "QAFGe1R5m1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# turn region, category, and sub-category columns into vectors of numbers\n",
        "region=np.unique(df['Region'], return_inverse=True)[1]\n",
        "category=np.unique(df['Category'], return_inverse=True)[1]\n",
        "subCategory=np.unique(df['Sub-Category'], return_inverse=True)[1]\n",
        "\n",
        "# turn quantity, discount, and profit columns into vectors of numbers\n",
        "quantity = df[\"Quantity\"].to_numpy()\n",
        "discount = df[\"Discount\"].to_numpy()\n",
        "profit = df[\"Profit\"].to_numpy()\n",
        "\n",
        "vectorizedDataset = np.empty((len(region), 5))\n",
        "labels = np.empty(len(region))\n",
        "\n",
        "# generate feature vectors\n",
        "for i in range(0, len(region)):\n",
        "  data = np.zeros((1, 5))\n",
        "  data[0][0] = region[i]\n",
        "  data[0][1] = category[i]\n",
        "  data[0][2] = subCategory[i]\n",
        "  data[0][3] = quantity[i]\n",
        "  data[0][4] = discount[i]\n",
        "\n",
        "  vectorizedDataset[i] = data\n",
        "\n",
        "  if (profit[i] > MEDIAN):\n",
        "    labels[i] = 1\n",
        "  else:\n",
        "    labels[i] = 0"
      ],
      "metadata": {
        "id": "Pb_9doGYVYXT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffles the data and splits it into the train, test, and validation sets\n",
        "train, validAndTest, trainLabels, validAndTestLabels = train_test_split(vectorizedDataset, labels, test_size=0.3, random_state=RANDOM_STATE)\n",
        "valid, test, validLabels, testLabels = train_test_split(validAndTest, validAndTestLabels, test_size=0.5, random_state=RANDOM_STATE)"
      ],
      "metadata": {
        "id": "HBTcec8KgELT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train))\n",
        "print(len(trainLabels))\n",
        "print()\n",
        "\n",
        "print(len(valid))\n",
        "print(len(validLabels))\n",
        "print()\n",
        "\n",
        "print(len(test))\n",
        "print(len(testLabels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5REpfAaDg1Mu",
        "outputId": "344cc569-4156-46e0-8d5e-e1a968e2ef1d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4146\n",
            "4146\n",
            "\n",
            "888\n",
            "888\n",
            "\n",
            "889\n",
            "889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainLabels = trainLabels.T"
      ],
      "metadata": {
        "id": "b9uyDLtJh_sK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machine"
      ],
      "metadata": {
        "id": "EAN0rNw6d5Im"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first model we generate is a Support Vector Machine (SVM).  SVM finds the optimal hyperplane to seperate the purchases above the median profit and those below the median profit in the feature space.  It works well when there is a clear margin of separation between classes, which I believe there might be given the \"above the median profit\" and \"below the median profit\" do not seem to overlap in terms of features.  SVM is also relatively memory efficient and quick to train.  However, if there is a lot of nose in the dataset, the algorithm will not be able to find the optimal hyperplane to separate the two classes.  If SVM performs poorly, I would assume this is the reason why."
      ],
      "metadata": {
        "id": "zkl7qRMXnbzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell specifies the hyperparameters for our SVM.  We perform a grid search across the penalty, alpha, and epochs hyperparameters.  This means that we try all possible combination of each hyperparameter, and record the combination that performs the best.  This approach will improve our model's performance, as the hyperparameters we find will likely be better than the default hyperparameters sklearn specifies."
      ],
      "metadata": {
        "id": "ZG1hT8GaqFEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PENALTY = [\"l2\", \"l1\", \"elasticnet\"] # determines the penalty term\n",
        "ALPHA = [0.01, 0.001, 0.0002, 0.0001, 0.0005, 0.00001] # determines the learning rate\n",
        "EPOCHS = [500, 750, 1000, 1250, 1500] # number of epochs for training\n",
        "\n",
        "# sklearn paritions its own validation set from the train data we pass in an and stops training once performance on\n",
        "# its validation set no longer improves after 5 epochs\n",
        "# this early stopping is important to prevent overfitting\n",
        "EARLY_STOPPING = True\n",
        "VALIDATION_FRACTION = 0.15 # fraction of training data to use for validation"
      ],
      "metadata": {
        "id": "HHa-BtvWQGS2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bestPenalty = \"l2\"\n",
        "bestAplha = 0.0001\n",
        "bestEpochs = 1000\n",
        "bestF1 = 0\n",
        "\n",
        "# grid search\n",
        "for penalty in PENALTY:\n",
        "  for alpha in ALPHA:\n",
        "    for epoch in EPOCHS:\n",
        "      # the default loss function for SGDClassifier generates an SVM\n",
        "      SVM = SGDClassifier(penalty=penalty, alpha=alpha, max_iter=epoch, early_stopping=EARLY_STOPPING, validation_fraction=VALIDATION_FRACTION, random_state=RANDOM_STATE)\n",
        "      SVM.fit(train, trainLabels)\n",
        "\n",
        "      # evalaute on validation set\n",
        "      predictions = SVM.predict(valid)\n",
        "      f1 = f1_score(validLabels, predictions, average=\"macro\")\n",
        "\n",
        "      if f1 > bestF1:\n",
        "        bestF1 = f1\n",
        "        bestPenalty = penalty\n",
        "        bestAplha = alpha\n",
        "        bestEpochs = epoch\n",
        "\n",
        "# best performing hyperparameters\n",
        "print(\"Best f1: \" + str(bestF1))\n",
        "print(\"Best epochs: \" + str(bestEpochs))\n",
        "print(\"Best alpha: \" + str(bestAplha))\n",
        "print(\"Best penalty: \" + str(bestPenalty))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUGnK2kFg3pK",
        "outputId": "6c9b87db-a373-440d-e789-b483e66ca54b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best f1: 0.699212165409399\n",
            "Best epochs: 500\n",
            "Best alpha: 0.0005\n",
            "Best penalty: elasticnet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SVM = SGDClassifier(penalty=bestPenalty, alpha=bestAplha, max_iter=bestEpochs, early_stopping=EARLY_STOPPING, validation_fraction=VALIDATION_FRACTION, random_state=RANDOM_STATE) # this is a linear SVM\n",
        "SVM.fit(train, trainLabels)\n",
        "predictions = SVM.predict(test)"
      ],
      "metadata": {
        "id": "kOS-7XE5iMUZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation of best-performing hyperparameters\n",
        "print(classification_report(testLabels, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3id_8bXiVAT",
        "outputId": "f6a7f7f6-4862-4c45-ce1a-37f2ba70655b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.83      0.75       499\n",
            "         1.0       0.70      0.52      0.60       390\n",
            "\n",
            "    accuracy                           0.69       889\n",
            "   macro avg       0.69      0.67      0.68       889\n",
            "weighted avg       0.69      0.69      0.68       889\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model performs reasonably well, with an f1 of 0.68 and overall accuracy of 0.69.  A deep learning approach would likely perform better; however, the advantage of an SVM is that it is far more lightweight and memory efficient.  If a superstore were implementing a model to predict at order time if that order would make above or below the median profit, an SVM would likely be a better choice for its fast performance across thousands of orders.  As this model does substantially better than a random guess, it is at least usable for our classification task."
      ],
      "metadata": {
        "id": "CQjsULXHtlrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "gkH6-7DSd0y8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second model we try is Logistic Regression, which is essentially a single layer of a neural network.  It works well for binary classification tasks (which our classification task is), and serves as a solid performance baseline for other ML approaches on this task.  One advantage of logistic regression is that it is highly interpretable.  The weights of logistic regression each correspond to an input feature, so we can use them to see which features are weighted most heavily when predicting profit.  This could also provide useful information to the superstore on which features most heavily infouence weather a purchase will make above the median profit or not."
      ],
      "metadata": {
        "id": "4e78wU9vvHYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform another grid search for the logistic regression hyperparameters."
      ],
      "metadata": {
        "id": "xtXQ7KMcwM90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR_PENALTY = [\"l2\", \"l1\", \"elasticnet\"] # determines the penalty term\n",
        "LR_ALPHA = [0.01, 0.001, 0.0002, 0.0001, 0.0005, 0.00001] # determines the learning rate\n",
        "LR_EPOCHS = [500, 750, 1000, 1250, 1500] # number of epochs for training"
      ],
      "metadata": {
        "id": "18IDwh5KeG3-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_bestPenalty = \"l2\"\n",
        "lr_bestAplha = 0.0001\n",
        "lr_bestEpochs = 1000\n",
        "lr_bestF1 = 0\n",
        "\n",
        " # grid search\n",
        "for penalty in LR_PENALTY:\n",
        "  for alpha in LR_ALPHA:\n",
        "    for epoch in LR_EPOCHS:\n",
        "      # we specify log_loss to train a logistic regression classifier\n",
        "      LogReg = SGDClassifier(loss = 'log_loss', penalty=penalty, alpha=alpha, max_iter=epoch, early_stopping=EARLY_STOPPING, validation_fraction=VALIDATION_FRACTION, random_state=RANDOM_STATE)\n",
        "      LogReg.fit(train, trainLabels)\n",
        "\n",
        "      # evaluation on the validation set\n",
        "      predictions = LogReg.predict(valid)\n",
        "      f1 = f1_score(validLabels, predictions, average=\"macro\")\n",
        "\n",
        "      if f1 > lr_bestF1:\n",
        "        lr_bestF1 = f1\n",
        "        lr_bestPenalty = penalty\n",
        "        lr_bestAplha = alpha\n",
        "        lr_bestEpochs = epoch\n",
        "\n",
        "# best performing hyperparameters\n",
        "print(\"Best f1: \" + str(lr_bestF1))\n",
        "print(\"Best epochs: \" + str(lr_bestEpochs))\n",
        "print(\"Best alpha: \" + str(lr_bestAplha))\n",
        "print(\"Best penalty: \" + str(lr_bestPenalty))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_EaVYIDezjO",
        "outputId": "b1c6eefe-466e-45fe-f081-a6a5412eb073"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best f1: 0.6982494097882874\n",
            "Best epochs: 500\n",
            "Best alpha: 0.0001\n",
            "Best penalty: l1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LogReg = SGDClassifier(loss = 'log_loss', penalty=lr_bestPenalty, alpha=lr_bestAplha, max_iter=lr_bestEpochs, early_stopping=EARLY_STOPPING, validation_fraction=VALIDATION_FRACTION, random_state=RANDOM_STATE) # this is a linear SVM\n",
        "LogReg.fit(train, trainLabels)\n",
        "predictions = LogReg.predict(test)"
      ],
      "metadata": {
        "id": "kvwWpEHVfGEG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation of best-performing hyperparameters\n",
        "print(classification_report(testLabels, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dTzRxeGfP_Q",
        "outputId": "5685df2a-62c6-4729-e8b9-e56ca52de756"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      0.67      0.71       499\n",
            "         1.0       0.63      0.71      0.66       390\n",
            "\n",
            "    accuracy                           0.69       889\n",
            "   macro avg       0.69      0.69      0.68       889\n",
            "weighted avg       0.69      0.69      0.69       889\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of what we said in the SVM results section holds true here as well.  Logistic regression performs substantially better than a random guess, and would likely be usable for our classification task.  Like SVM, logistic regression is lightweight and would almost assuredly perform inference quicker than a neural network.  However, a neural network would likely make predictions more accurately."
      ],
      "metadata": {
        "id": "55gVuOIDwl9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(LogReg.coef_) # logistic regression weights"
      ],
      "metadata": {
        "id": "tVYwREYZhszu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6ae055-d2f9-46de-dba7-23beefb33532"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  -1.5645983     6.78497593    0.            4.77404507 -140.38173997]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above cell shows the weights of our logistic regression model.  We see that discount has a strong negative relationship with profit, which makes sense (the higher the discount, the less the superstore makes).  Quantity has a positive relationshup with profit which agin makes sense; the more a customer buys, the greater the superstore's overall profit.  Region and product category have a negative and positive relationship with profit respectively (as we have defined the numerical labels for regions and product categories).  This is very useful information for the superstore; they might want to analyse their best performing product categories and regions they sell to, and reduce their operations to only those categories/regions.  Surprisingly, product sub-category has no relationship with profit.  I assume this is because the other features are far better predictiors of profit than product sub-category is; however, given the statistical test showing profit was correlated with product sub-category, I am surprised by this result."
      ],
      "metadata": {
        "id": "FqoIZ4IpxNnU"
      }
    }
  ]
}